---
layout: archive
permalink: /
title: "Background"
excerpt: "basic information"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<html>
<head>
  <!-- <link rel="stylesheet" type="text/css" href="../_sass/_base.scss"></link> -->
  <style type="text/css">
  </style>
</head>
<body>

{% include base_path %}
<hr>

<!-- <h2>Background</h2> -->
<!-- <p><font size=2>Last Updated: 05/12/2024</font></p> -->
<!-- <font size=5>Basic Information</font> -->
<p></p>
<!-- <p><b>Please reach out to me</b> via tianyiz0423@gmail.com or zty@upenn.edu. <b>NOT</b> zty@seas.upenn.edu</p> -->

<p>I am the ideal match for your needs, if you're seeking students with a robust foundation in:<br>
Natural Language Processing, Human Cognition, and STEM Education.</p>

<!-- <p><font size=3>I am passionate about developing anthropomorphic agents to support underprivileged populations. Specifically, my endeavors <b>modify current black-box language models (LMs) to emulate human understanding and reasoning procedure</b>. Humans evolve sophisticated intelligence through explicit structured knowledge and symbolic systems (Tulving, E., 1985). In contrast, a significant challenge with current LMs, including the SOTA GPT-4o, is their inability to automatically construct such structured and symbolic representations within the neural networks, as humans do. This deficiency, lacking intermediate symbolic representations to bridge neurons and language outputs, leads to widely criticized unreliable reasoning in LMs.</font></p>

<p><font size=3>To alleviate it, my research directs LMs to construct and reason with structured and symbolic representations. My research trajectory begins with event extraction through question-answering and synthetic data augmentation techniques (Proj.4), followed by constructing event schemas using human-computer interaction (Proj.3). These efforts assist models to interpret structured knowledge graphs from unstructured text. Subsequently, I investigate the translation of natural language into agent-executable symbolic language utilizing a human cognition theory named ZPD (Proj.2). This work enhances the model’s reliable reasoning capabilities via symbolic representations. Currently, my work explores pretraining LMs using a reconstruction loop that integrates both natural language and knowledge graphs (Proj.1). This approach embeds encoding-decoding skill, akin to human learning processes, into the LMs in the pretraining phase. Collectively, my research aims to advance artificial agents with human-like thinking abilities.</font></p> -->

<p>I am passionate about developing anthropomorphic agents to support underprivileged populations. My endeavors <b>modify current black-box language models (LMs) to emulate human understanding and reasoning procedure</b>. Specifically, my research directs LMs to construct and reason with structured and symbolic representations from unstructured text.</p>

<p>To know more about my research experience, please check my <a href="https://tianyi0608.github.io/tianyizhang/files/cv.pdf">CV</a> and <a href="https://tianyi0608.github.io/tianyizhang/files/SOP-Tianyi Zhang-2025.pdf">SOP</a>.</p>

<p>My <b>research efforts</b> include:
<ol>
  <li>Pretraining language models through a text-knowledge graph reconstruction loop.</li>
  <li>Translating natural language text into symbolic representations.</li>
  <li>Extracting events and inducing event schemas from natural language text.</li>
</ol>
</p>

<p>My <b>research interests</b> include:
<ol>
  <li>Reasoning in Natural/Symbolic Language</li>
  <li>Language Model Pretraining</li>
  <li>Interdisciplinary in NLP, CV, and Robotics</li>
</ol>
</p>


<!-- To know more about my research, please check my research projects [<a href="https://tianyi0608.github.io/tianyizhang/files/research_projects.pdf">slides</a>] [<a href="https://youtu.be/e0t4urFML2A">video recording</a>].
If you are interested in my work, feel free to <a href="mailto: zty@seas.upenn.edu">email</a> me.</font></p> -->


<!-- **<font size=5>Research Interests:</font>**
1. Reasoning in Natural/Symbolic Language
2. Language Model Pretraining
3. Interdisciplinary in NLP, CV and Robotics

**<font size=5>Expertise:</font>**
1. **Learning Science** and **Cognitive Science** (6 years of experience, B.S., M.Ed)
    - Human Learning and Knowledge Storage
    - Learning Task Design
2. **Natural Language Processing** (4 years of experience, MSE)
    - Event Extraction, Schema Induction
    - Reasoning in Natural and Symbolic Language<br>
 -->

 <!-- <hr> -->

<h2>Publications</h2><hr>

<ol> 
<!--   <li>
    <span style="float: left;">Pretraining Language Models with NL-KG-NL Reconstruction Loop.</span>
    <span style="float: right;">In writing 2025</span>
    <br style="clear: both;" />
    <span style="float: left; font-size: 16px;"><b>Zhang, T.</b>, Mai, F., Pan, R., Flek, L.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://tianyi0608.github.io/tianyizhang/files/Text_KG_loop.pdf">paper</a></span>  <!-- <a href="">poster</a>  <a href="">oral</a> -->
    <br style="clear: both;" />
  </li> -->
  
  <li>
    <span style="float: left;">Effective Domain Adaptation of Instruction-Tuned LLMs for Knowledge-Intensive Tasks.</span>
    <span style="float: right;">In submission 2025</span>
    <br style="clear: both;" />
    <span style="float: left; font-size: 16px;"><b>Zhang, T.</b>, Mai, F., Flek, L.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://tianyi0608.github.io/tianyizhang/files/Domain_Adaptation_for_Instruction_Tuned_LLMs__EMNLP_1.pdf">paper</a></span>  <!-- <a href="">poster</a>  <a href="">oral</a> -->
    <br style="clear: both;" />
  </li>

  <li>
    <span style="float: left;">PROC2PDDL: Open-Domain Planning Representations from Texts.</span>
    <span style="float: right;">NLRSE@ACL 2024</span>
    <br style="clear: both;" />
    <span style="float: left; font-size: 16px;"><b>Zhang, T.</b>*, Zhang, L.*, Hou, Z., Wang, Z., Gu, Y., Clark, P., Callison-Burch, C., and Tandon, N.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://aclanthology.org/2024.nlrse-1.2.pdf">paper</a>    <a href="https://tianyi0608.github.io/tianyizhang/files/proc2pddl-poster.pdf">poster</a>    <a href="https://underline.io/lecture/104074-proc2pddl-open-domain-planning-representations-from-texts">oral</a></span>
    <br style="clear: both;" />
  </li>

  <li>
    <span style="float: left;">PDDLEGO: Iterative Planning in Textual Environments.</span>
    <span style="float: right;">*SEM 2024</span>
    <br style="clear: both;" />
    <span style="float: left;  font-size: 16px;">Zhang, L., Jansen, P., <b>Zhang, T.</b>, Clark, P., Callison-Burch, C., Tandon, N.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://aclanthology.org/2024.starsem-1.17v2.pdf">paper</a>    <a href="https://underline.io/lecture/96705-pddlego-iterative-planning-in-textual-environments">oral</a></span>
    <br style="clear: both;" />
  </li>

  <li>
    <span style="float: left;">WorldWeaver: Procedural World Generation for Text Adventure Games.</span>
    <span style="float: right;">Wordplay@ACL 2024</span>
    <br style="clear: both;" />
    <span style="float: left; font-size: 16px;">Jin, M., Kaul, M., Ramakrishnan, S., Jain, H., Chandrawat, S., Agarwal, I., <b>Zhang, T.</b>, Zhu, A., Callison-Burch, C.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://www.cis.upenn.edu/~ccb/publications/worldweaver.pdf">paper</a></span>
    <br style="clear: both;" />
  </li>

  <li>
    <span style="float: left;">Human-in-the-Loop Schema Induction.</span>
    <span style="float: right;">ACL Demo 2023</span>
    <br style="clear: both;" />
    <span style="float: left; font-size: 16px;"><b>Zhang, T.</b>*, Tham, I. *, Hou, Z. *, Ren, J., Zhou, L., Xu, H., Zhang, L., Martin, L., Dror, R., Li, S., Ji, H., Palmer, M., Brown, S., Suchocki, R., and Callison-Burch, C.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://aclanthology.org/2023.acl-demo.1.pdf">paper</a>    <a href="https://tianyi0608.github.io/tianyizhang/files/schema-induction-poster.pdf">poster</a>    <a href="https://underline.io/lecture/78228-human-in-the-loop-schema-induction">oral</a></span>
    <br style="clear: both;" />
  </li>

  <li>
    <span style="float: left;">Question-Answering Data Augmentation for Argument Role Labeling.</span>
    <span style="float: right;">2022</span>
    <br style="clear: both;" />
    <span style="float: left; font-size: 16px;"><b>Zhang, T.</b>, Sulem, E., Roth, D.</span>
    <br style="clear: both;" />
    <span style="float: left;"><a href="https://tianyi0608.github.io/tianyizhang/files/AE-QG.pdf">paper</a></span>
    <br style="clear: both;" />
  </li>
</ol>


<h2>Education</h2><hr>

<ul>
    <li>
        MSE in Data Science, Jan. 2021 - Dec. 2022<br>
        <font size="3">University of Pennsylvania, Philadelphia, America</font>
    </li>
    <li>
        M.Ed in Learning Science and Technology, Sept. 2018 - Dec. 2019<br>
        <font size="3">University of Pennsylvania, Philadelphia, America</font>
    </li>
    <li>
        B.S in Educational Technology, Sept. 2014 - Jun. 2018<br>
        <font size="3">Beijing Normal University, Beijing, China</font>
    </li>
</ul>

</body>
</html>

<!--   - MSE in Data Science,   Jan. 2021 - Dec. 2022<br>
    <font size=3>University of Pennsylvania, Philadelphia, America</font>
  - M.Ed in Learning Science and Technology,   Sept. 2018 - Dec. 2019<br>
    <font size=3>University of Pennsylvania, Philadelphia, America</font>
  - B.S in Educational Technology,   Sept. 2014 - Jun. 2018<br>
    <font size=3>Beijing Normal University, Beijing, China</font> -->

<!-- # Research Experience
1. **Research Assistant: NLP Group at UPenn**, May. 2022 - Jul. 2023
2. **Research Assistant: Cognitive Computation Group at UPenn**, Mar. 2020 – Dec. 2022 -->

<!--    - Event **schema generation** using GPT-3
      <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li><strong>Design the scaffold prompts</strong> (cause, plan, procedure, effect, etc.) for GPT-3 to generate inclusive events</li>
        <li>Apply SRL and constituency parsing to summarize and <strong>extract structured events</strong></li>
        <li>Build schema graphs by adding temporal relations to the events</li>
        <li><strong>Ground the graph nodes</strong> into the ontology through the semantic inference or the semantic similarity</li>
        <li>Outcome: "Human-in-the-Loop Schema Induction" paper</li>
        </ul>
      </font></blockquote>
    - Natural language to **symbolic language** translation for **reasoning**
      <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li>Focus on open-domain natural language (wikiHow) to symbolic language (PDDL) generation with GPT-4</li>
        <li><strong>Decompose the task</strong> into three stages: extraction, inference, and translation</li>
        <li>Identify strong <strong>text extraction and entity states inference</strong> abilities with increasingly complex wikiHow text (~5000 words)</li>
        <li>Acknowledge a weak <strong>translation capability</strong> to predefined symbolic pattern</li>
        <li>Improve the performance using <strong>CoT</strong> and <strong>instructions</strong> on translation.</li>
        <li>Outcome:"PROC2PDDL" paper</li>
        </ul>
      </font></blockquote> -->


