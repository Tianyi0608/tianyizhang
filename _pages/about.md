---
permalink: /
title: "Basic Information"
excerpt: "basic information"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- <p><font size=2>Last Updated: 05/12/2024</font></p> -->
<!-- <font size=5>Basic Information</font> -->
<!-- I am in search of a Ph.D. position in 2024, preferably in America.<br> -->
<p><font size=3>
I am the ideal match for your needs, if you're seeking students with a robust foundation in:<br>
NLP (LLMs), human cognition, and education (including computer science, STEM).</font></p>
<!-- I am <b>fascinated by reasoning</b> and dedicate to <b>causality described in pre- and post-conditions</b> (referring to the PROC2PDDL paper). If you are interested in this, feel free to email me.&#128515; -->

<p><font size=3>I am passionate about developing anthropomorphic agents to support underprivileged populations. Specifically, my endeavors <b>modify current black-box language models (LMs) to emulate human understanding and reasoning procedure</b>. Humans evolve sophisticated intelligence through explicit structured knowledge and symbolic systems (Tulving, E., 1985). In contrast, a significant challenge with current LMs, including the SOTA GPT-4o, is their inability to automatically construct such structured and symbolic representations within the neural networks, as humans do. This deficiency, lacking intermediate symbolic representations to bridge neurons and language outputs, leads to widely criticized unreliable reasoning in LMs.</font></p>

<p><font size=3>To alleviate it, my research directs LMs to construct and reason with structured and symbolic representations. My research trajectory begins with event extraction through question-answering and synthetic data augmentation techniques (Proj.4), followed by constructing event schemas using human-computer interaction (Proj.3). These efforts assist models to interpret structured knowledge graphs from unstructured text. Subsequently, I investigate the translation of natural language into agent-executable symbolic language utilizing a human cognition theory named ZPD (Proj.2). This work enhances the model’s reliable reasoning capabilities via symbolic representations. Currently, my work explores pretraining LMs using a reconstruction loop that integrates both natural language and knowledge graphs (Proj.1). This approach embeds encoding-decoding skill, akin to human learning processes, into the LMs in the pretraining phase. Collectively, my research aims to advance artificial agents with human-like thinking abilities.</font></p>
<!-- <a href="https://tianyi0608.github.io/tianyizhang/files/proc2pddl.pdf"> -->

<!-- <p><font size=3>I am currently working on a project for Itinerary Planning, utilizing LM-assisted Symbolic Reasoning. This innovative approach -- acquiring and applying symbolic knowledge, is capable of solving complex planning tasks accurately and faithfully. For a clearer view, please refer to the <a href="https://tianyi0608.github.io/tianyizhang/files/itinerary_planning.png">system flowchart</a>.</font></p> -->

<p><font size=3>To know more about my experience, please check my <a href="https://tianyi0608.github.io/tianyizhang/files/CV_NLP_TianyiZhang_241126.pdf">CV</a>, and <a href="https://tianyi0608.github.io/tianyizhang/files/SOP-Tianyi Zhang-2025.pdf">SOP</a>.<br>
To know more about my research, please check my research projects. [<a href="https://tianyi0608.github.io/tianyizhang/files/research_projects.pdf">slides</a>] [<a href="https://youtu.be/e0t4urFML2A">video recording</a>]<br>
<!-- <button onclick="window.location.href='https://youtu.be/e0t4urFML2A';">video recording</button> -->
If you are interested in my work, feel free to <a href="mailto: zty@seas.upenn.edu">email</a> me.</font></p>

------

<!-- **<font size=5>Highest Education:</font>**
&emsp;MSE in Data Science at University of Pennsylvania;  GPA: 3.97 -->

**<font size=5>Research Interests:</font>**
1. Reasoning in Natural/Symbolic Language
2. Interdisciplinary in NLP, CV and Robotics

**<font size=5>Expertise:</font>**
1. **Learning Science** and **Cognitive Science** (6 years of experience, B.S., M.Ed)
    - Human Learning and Knowledge Storage
    - Learning Task Design
2. **Natural Language Processing** (3 years of experience, MSE)
    - Event Extraction
    - Event Reasoning in Natural and Symbolic Language<br>
      <font size=3>e.g. Schema Generation, Event and Entity Pre- and Post-condition Generation</font>

------

# Publications

<div style="display: flex; justify-content: space-between;">
  <span>Content from the left</span>
  <span>2024</span>
</div>

<ol>
  <li>
    <span style="float: left;">Some content</span>
    <span style="float: right;">2022</span>
    <br style="clear: both;" />
  </li>
  <li>
    <span style="float: left;">Another content</span>
    <span style="float: right;">2023</span>
    <br style="clear: both;" />
  </li>
  <li>
    <span style="float: left;">More content</span>
    <span style="float: right;">2024</span>
    <br style="clear: both;" />
  </li>
</ol>



- [Pretraining Language Models with NL-KG-NL Reconstruction Loop.]() **Zhang, T.**, Mai, F., Flek, L., (2024). Paper in writing.
- [Human-in-the-Loop Schema Induction.](https://aclanthology.org/2023.acl-demo.1.pdf)
    **Tianyi Zhang\***, Isaac Tham\*, Zhaoyi Hou\*, Jiaxuan Ren, Liyang Zhou, Hainiu Xu, Li Zhang, Lara J. Martin, Rotem Dror, Sha Li, Heng Ji, Martha Palmer, Susan Brown, Reece Suchocki, Chris Callison-Burch. ACL Demo 2023

- [Question-Answer Data Augmentation for Argument Role Labeling.](https://tianyi0608.github.io/tianyizhang/files/AE-QG.pdf)
    **Tianyi Zhang**, Elior Sulem, Dan Roth. 2023, In submission

- [PROC2PDDL: Towards Open-Domain Symbolic Planning.](https://tianyi0608.github.io/tianyizhang/files/proc2pddl.pdf)
    **Tianyi Zhang\***, Li Zhang\*, Zhaoyi Hou, Ziyu Wang, Yuling Gu, Peter Clark, Chris Callison-Burch, Niket Tandon. 2023, In submission
    <!-- <br>Not publicly available currently. -->
<!-- , please email me if you are interested in it -->
<!-- https://tianyi0608.github.io/tianyizhang/files/proc2pddl.pdf -->

------

# Education
  - MSE in Data Science,   Jan. 2021 - Dec. 2022<br>
    <font size=3>University of Pennsylvania, Philadelphia, America</font>
  - M.Ed in Learning Science and Technology,   Sept. 2018 - Dec. 2019<br>
    <font size=3>University of Pennsylvania, Philadelphia, America</font>
  - B.S in Educational Technology,   Sept. 2014 - Jun. 2018<br>
    <font size=3>Beijing Normal University, Beijing, China</font>

------

# Research Experience
1. **Research Assistant: NLP Group at UPenn**, May. 2022 - Jul. 2023
2. **Research Assistant: Cognitive Computation Group at UPenn**, Mar. 2020 – Dec. 2022

<!--    - Event **schema generation** using GPT-3
      <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li><strong>Design the scaffold prompts</strong> (cause, plan, procedure, effect, etc.) for GPT-3 to generate inclusive events</li>
        <li>Apply SRL and constituency parsing to summarize and <strong>extract structured events</strong></li>
        <li>Build schema graphs by adding temporal relations to the events</li>
        <li><strong>Ground the graph nodes</strong> into the ontology through the semantic inference or the semantic similarity</li>
        <li>Outcome: "Human-in-the-Loop Schema Induction" paper</li>
        </ul>
      </font></blockquote>
    - Natural language to **symbolic language** translation for **reasoning**
      <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li>Focus on open-domain natural language (wikiHow) to symbolic language (PDDL) generation with GPT-4</li>
        <li><strong>Decompose the task</strong> into three stages: extraction, inference, and translation</li>
        <li>Identify strong <strong>text extraction and entity states inference</strong> abilities with increasingly complex wikiHow text (~5000 words)</li>
        <li>Acknowledge a weak <strong>translation capability</strong> to predefined symbolic pattern</li>
        <li>Improve the performance using <strong>CoT</strong> and <strong>instructions</strong> on translation.</li>
        <li>Outcome:"PROC2PDDL" paper</li>
        </ul>
      </font></blockquote>

2. **Research Assistant: Cognitive Computation Group at UPenn**, Mar. 2020 – Dec. 2022   

    - Event **trigger identification** and **classification** using **sequence tagging**
      <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li>Build 1-of-N (72) joint model and <strong>BIO identification + event type classification</strong> pipeline model</li>
        <li><strong>Transfer learning</strong> with target language auxiliary dataset, e.g. OntoNotes Arabic</li>
        </ul>
      </font></blockquote>
    - Event **argument identification** and **classification** using **QA**
        <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li><strong>Design fixed questions</strong> for each argument role and convert argument role labeling task to question answering task</li>
        <li>Build has-and-no-answer joint model and <strong>has/no answer classification + has answer identification</strong> pipeline model</li>
        <li>Transfer learning with auxiliary QA dataset, e.g. SQuAD, QAMR</li>
        </ul>
      </font></blockquote>
    - Event **data augmentation** using **answer extraction (AE)** and **question generation (QG)**
        <blockquote style="font-style:normal;"><font size=3>
        <ul>
        <li>Train <strong>AE-QG T5 model</strong> to extract QA pairs from unlabeled event text</li>
        <li>Train <strong>AEwSRL-QG Bert-T5 model</strong> to extract QA pairs from unlabeled event text</li>
        <li><strong>Evaluate on joint-QA and pipeline-QA model</strong> and prove effectiveness of the above data augmentation approach</li>
        <li>Outcome: "Argument Role Labelling Question-Answer Pair Data Augmentation" paper</li>
        </ul>
      </font></blockquote> -->


